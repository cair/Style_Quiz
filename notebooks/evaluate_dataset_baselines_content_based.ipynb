{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaborg15\\Python_projects\\Vibrent_Dataset_Collection\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(os.getcwd())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "from resources.constants import *\n",
    "\n",
    "pictures_df = pd.read_csv(PICTURE_TRIPLETS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "outfits_df = pd.read_csv(OUTFITS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "user_triplets_df = pd.read_csv(USER_ACTIVITY_TRIPLETS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "# Ensure tags are lists\n",
    "outfits_df[\"tag_categories\"] = outfits_df[\"tag_categories\"].apply(eval)\n",
    "outfits_df[\"outfit_tags\"] = outfits_df[\"outfit_tags\"].apply(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.load_baseline_resources\n",
    "import pickle\n",
    "from resources.constants import EMBEDDING_MODEL_DICT_PICKLE_PATH\n",
    "# loaded_embeddings_dict = src.load_baseline_resources.load_embeddings_form_folder()\n",
    "# pickle.dump(loaded_embeddings_dict, open(EMBEDDING_MODEL_DICT_PICKLE_PATH, \"wb\"))\n",
    "\n",
    "# Loading embeddings is expensive, so we save them to a pickle file\n",
    "loaded_embeddings_dict = pickle.load(open(EMBEDDING_MODEL_DICT_PICKLE_PATH, \"rb\"))\n",
    "\n",
    "\n",
    "pictures_df[\"embeddings\"] = pictures_df[\"picture.id\"].map(loaded_embeddings_dict)\n",
    "outfit_pictures_df = pictures_df.groupby(\"outfit.id\").agg({\"picture.id\": list, \"embeddings\": list}).reset_index()\n",
    "outfits_df[\"embeddings\"] = outfits_df[\"id\"].map(outfit_pictures_df.set_index(\"outfit.id\")[\"embeddings\"])\n",
    "na_embedding_outfit_ids = outfits_df[outfits_df[\"embeddings\"].isna()][\"id\"]\n",
    "outfits_df = outfits_df.dropna(subset=[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce group to rental triplets\n",
    "id_group_dict = outfits_df[[\"id\", \"group\"]].to_dict(orient=\"records\")\n",
    "id_group_dict = {x[\"id\"]: x[\"group\"] for x in id_group_dict}\n",
    "user_triplets_df[\"group\"] = user_triplets_df[\"outfit.id\"].map(id_group_dict)\n",
    "# Remove triplets with no embeddings\n",
    "user_triplets_df = user_triplets_df[~user_triplets_df[\"outfit.id\"].isin(na_embedding_outfit_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unique outfit found with groups ['group.8abe6af9eccc8b578c2ef59628f8b454'\n",
      " 'group.96f4cce22d4a236e0652c67fc9b18d12'\n",
      " 'group.8abe6af9eccc8b578c2ef59628f8b454'\n",
      " 'group.96f4cce22d4a236e0652c67fc9b18d12']\n"
     ]
    }
   ],
   "source": [
    "from src.prepare_train_test_splits import convert_user_orders_to_train_test_splits\n",
    "user_orders_df = user_triplets_df.groupby(\"customer.id\").agg({\"outfit.id\": list, \"group\":list, \"meta.validFrom\":list, \"derived.bookingTime\":list}).reset_index()\n",
    "user_orders_df[\"num_orders\"] = user_orders_df[\"outfit.id\"].apply(lambda x: len(x))\n",
    "user_orders_df = user_orders_df[user_orders_df[\"num_orders\"] > 1]\n",
    "\n",
    "user_splits_df, user_splits_unique_df = convert_user_orders_to_train_test_splits(user_orders_df, percentage_test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2215.000000\n",
       "mean       20.579684\n",
       "std        29.432111\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%        10.000000\n",
       "75%        25.000000\n",
       "max       250.000000\n",
       "Name: train_outfit_ids, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_splits_unique_df[\"train_outfit_ids\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2215.000000\n",
       "mean        8.371558\n",
       "std        12.488101\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         3.000000\n",
       "75%        10.000000\n",
       "max       106.000000\n",
       "Name: test_outfit_id, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_splits_unique_df[\"test_outfit_id\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bf8b8101874f0eaca70f2d85c6ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def build_tag_dict(tags, tag_categories):\n",
    "    tag_dict = {}\n",
    "    for tag, tag_category in zip(tags, tag_categories):\n",
    "        if tag_category not in tag_dict:\n",
    "            tag_dict[tag_category] = []\n",
    "        tag_dict[tag_category].append(tag)\n",
    "    return tag_dict\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "outfits_df[\"tag_dict\"] = outfits_df.progress_apply(lambda x: build_tag_dict(x[\"outfit_tags\"], x[\"tag_categories\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "all_tags = outfits_df[\"outfit_tags\"].values.tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot_encoded = mlb.fit_transform(all_tags)\n",
    "outfits_df[\"one_hot_encoded\"] = [np.array(oh_list) for oh_list in one_hot_encoded.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaborg15\\AppData\\Local\\Temp\\ipykernel_6332\\3560725389.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  input_embeddings = torch.tensor(input_embeddings.tolist(), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataframe (example)\n",
    "# outfits_df = pd.read_csv(\"path_to_your_dataframe.csv\")\n",
    "\n",
    "# Assuming your dataframe has the following columns:\n",
    "# \"one_hot_encoded\" and \"mean_embeddings\"\n",
    "# Convert them to numpy arrays\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def concatenate_embeddings(oh_embeddings, image_embeddings, oh_weighting):\n",
    "    oh_embeddings = np.array(oh_embeddings) * oh_weighting\n",
    "    return np.concatenate((oh_embeddings, image_embeddings))\n",
    "\n",
    "outfits_df[\"mean_embeddings\"] = outfits_df[\"embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "#one_hot_encoded = np.array(outfits_df[\"one_hot_encoded\"].tolist())\n",
    "#mean_embeddings = np.array(outfits_df[\"mean_embeddings\"].tolist())\n",
    "\n",
    "outfits_df[\"concatenated_embeddings\"] = outfits_df.apply(lambda x: concatenate_embeddings(x[\"one_hot_encoded\"], x[\"mean_embeddings\"], oh_weighting=4), axis=1)\n",
    "input_embeddings = outfits_df[\"concatenated_embeddings\"]#np.concatenate((one_hot_encoded, mean_embeddings), axis=1)\n",
    "input_embeddings = torch.tensor(input_embeddings.tolist(), dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a5314af7984ecc96c4527a14c31f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.1251\n",
      "Epoch [2/3], Loss: 0.1069\n",
      "Epoch [3/3], Loss: 0.1113\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Assuming the input is normalized between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Define the dimensions\n",
    "input_dim = input_embeddings.shape[1]\n",
    "hidden_dim = 2048  # You can adjust this as needed\n",
    "latent_dim = 512   # You can adjust this as needed\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = Autoencoder(input_dim, hidden_dim, latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    permutation = torch.randperm(input_embeddings.size()[0])\n",
    "    \n",
    "    for i in range(0, input_embeddings.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_inputs = input_embeddings[indices]\n",
    "\n",
    "        # Forward pass\n",
    "        encoded, decoded = model(batch_inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(decoded, batch_inputs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), 'autoencoder_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m         encoded, decoded \u001b[38;5;241m=\u001b[39m model(input_embeddings)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n\u001b[1;32m---> 10\u001b[0m outfit_embeddings \u001b[38;5;241m=\u001b[39m get_outfit_embeddings(outfits_df, \u001b[43mmodel\u001b[49m)\n\u001b[0;32m     11\u001b[0m outfits_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutfit_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outfit_embeddings]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mstack(outfits_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutfit_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def get_outfit_embeddings(outfits_df, model):\n",
    "    one_hot_encoded = np.array(outfits_df[\"one_hot_encoded\"].tolist())\n",
    "    mean_embeddings = np.array(outfits_df[\"mean_embeddings\"].tolist())\n",
    "    input_embeddings = np.concatenate((one_hot_encoded, mean_embeddings), axis=1)\n",
    "    input_embeddings = torch.tensor(input_embeddings, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        encoded, decoded = model(input_embeddings)\n",
    "    return encoded\n",
    "\n",
    "outfit_embeddings = get_outfit_embeddings(outfits_df, model)\n",
    "outfits_df[\"outfit_embeddings\"] = [x.numpy() for x in outfit_embeddings]\n",
    "print(np.stack(outfits_df[\"outfit_embeddings\"].values).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "NUM_ITEMS = 100\n",
    "\n",
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def get_nearest_neighbors_batch(embeddings, nn, num_items, index_to_id):\n",
    "    distances, indices = nn.kneighbors(embeddings, n_neighbors=num_items+1)\n",
    "    ids = [[index_to_id[i] for i in idx[1:]] for idx in indices]\n",
    "    distances = [dist[1:] for dist in distances]\n",
    "    return ids, distances\n",
    "\n",
    "\n",
    "def predict_nearest_neighbors(df, outfits_df, embeddings_column=\"embeddings\", subset_length=-1):\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    index_to_outfit_dict = {i: outfit_id for i, outfit_id in enumerate(outfits_df[\"id\"].values)}\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    index_to_group_dict = {i: group for i, group in enumerate(outfits_df[\"group\"].values)}\n",
    "\n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=NUM_ITEMS+1, metric=\"cosine\")\n",
    "    embeddings = np.stack(outfits_df[embeddings_column].values)\n",
    "    nearest_neighbors.fit(embeddings)\n",
    "\n",
    "    id_embeddings = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "    group_embeddings = np.stack(df[\"rental_history_group_embedding\"].values)\n",
    "\n",
    "    id_predictions, id_distances = get_nearest_neighbors_batch(id_embeddings, nearest_neighbors, NUM_ITEMS, index_to_outfit_dict)\n",
    "    group_predictions, group_distances = get_nearest_neighbors_batch(group_embeddings, nearest_neighbors, NUM_ITEMS, index_to_group_dict)\n",
    "\n",
    "    df[\"id_prediction\"], df[\"id_prediction_distances\"] = id_predictions, id_distances\n",
    "    df[\"group_prediction\"], df[\"group_prediction_distances\"] = group_predictions, group_distances\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_nearest_neighbors_images(df, outfits_df, embeddings_column=\"embeddings\", subset_length=-1):\n",
    "    outfits_df[\"mean_embeddings\"] = outfits_df[embeddings_column].apply(lambda x: get_mean_embedding(x))\n",
    "\n",
    "    return predict_nearest_neighbors(df, outfits_df, embeddings_column=\"mean_embeddings\", subset_length=subset_length)\n",
    "\n",
    "# Apply to dataframes\n",
    "tqdm.pandas()\n",
    "\n",
    "# Tag based predictions\n",
    "user_splits_df = predict_nearest_neighbors(user_splits_df, outfits_df, embeddings_column=\"one_hot_encoded\", subset_length=-1)\n",
    "user_splits_unique_df = predict_nearest_neighbors(user_splits_unique_df, outfits_df, embeddings_column=\"one_hot_encoded\", subset_length=-1)\n",
    "\n",
    "# Image based predictions\n",
    "# user_splits_df = predict_nearest_neighbors_images(user_splits_df, outfits_df, embeddings_column=\"embeddings\", subset_length=-1)\n",
    "# user_splits_unique_df = predict_nearest_neighbors_images(user_splits_unique_df, outfits_df, embeddings_column=\"embeddings\", subset_length=-1)\n",
    "\n",
    "# Combined predictions\n",
    "# user_splits_df = predict_nearest_neighbors(user_splits_df, outfits_df, embeddings_column=\"outfit_embeddings\", subset_length=-1)\n",
    "# user_splits_unique_df = predict_nearest_neighbors(user_splits_unique_df, outfits_df, embeddings_column=\"outfit_embeddings\", subset_length=-1)\n",
    "\n",
    "# Concat predictions\n",
    "# user_splits_df = predict_nearest_neighbors(user_splits_df, outfits_df, embeddings_column=\"concatenated_embeddings\", subset_length=-1)\n",
    "# user_splits_unique_df = predict_nearest_neighbors(user_splits_unique_df, outfits_df, embeddings_column=\"concatenated_embeddings\", subset_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_hit_rate_at_100       0.309567\n",
       "id_hit_rate_at_10        0.074007\n",
       "group_hit_rate_at_100    0.328069\n",
       "group_hit_rate_at_10     0.097924\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id_hit_rate_at_100       0.288036\n",
       "id_hit_rate_at_10        0.062302\n",
       "group_hit_rate_at_100    0.284876\n",
       "group_hit_rate_at_10     0.065914\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def evaluate_hit_rate_at_n(test_id, predicted_ids, n=10):\n",
    "    if predicted_ids is np.nan:\n",
    "        print(f\"None prediction for {test_id}!\")\n",
    "        return 0\n",
    "    predicted_ids = predicted_ids[:n]\n",
    "    if type(test_id) == str or type(test_id) == np.str_:\n",
    "        if test_id in predicted_ids:\n",
    "            #print(f\"Hit at {n} for {test_id} in {predicted_ids}\")\n",
    "            return 1\n",
    "    elif type(test_id) == list or type(test_id) == np.ndarray:\n",
    "        for outfit_id in test_id:\n",
    "            if outfit_id in predicted_ids:\n",
    "                return 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type {type(test_id)}\")\n",
    "    return 0\n",
    "\n",
    "HIT_RATE_COLUMNS = [\"id_hit_rate_at_100\", \"id_hit_rate_at_10\", \"group_hit_rate_at_100\", \"group_hit_rate_at_10\"]\n",
    "def evaluate_df_hit_rate_at_n(df, n=10):\n",
    "    df[\"id_hit_rate_at_100\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_outfit_id\"], x[\"id_prediction\"], n=100), axis=1)\n",
    "    df[\"id_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_outfit_id\"], x[\"id_prediction\"], n=10), axis=1)\n",
    "    df[\"group_hit_rate_at_100\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_group\"], x[\"group_prediction\"], n=100), axis=1)\n",
    "    df[\"group_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n",
    "    display(df[HIT_RATE_COLUMNS].mean())\n",
    "    return df\n",
    "\n",
    "\n",
    "user_splits_df = evaluate_df_hit_rate_at_n(user_splits_df, n=10)\n",
    "user_splits_unique_df = evaluate_df_hit_rate_at_n(user_splits_unique_df, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Embed Ind & 0.0740 & 0.3096 & 0.0623 & 0.2880 \\\\\n",
      "Tag Embed Groups & 0.0979 & 0.3281 & 0.0659 & 0.2849 \\\\\\hline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyperclip\n",
    "\n",
    "def format_dicts_into_latex(all_dict, ind_dict, precision=4, run_name=\"Random\"):\n",
    "    first_row = f\"{run_name} Ind & {all_dict['id_hit_rate_at_10']:.{precision}f} & {all_dict['id_hit_rate_at_100']:.{precision}f} & {ind_dict['id_hit_rate_at_10']:.{precision}f} & {ind_dict['id_hit_rate_at_100']:.{precision}f} \\\\\\\\\"\n",
    "    second_row = f\"{run_name} Groups & {all_dict['group_hit_rate_at_10']:.{precision}f} & {all_dict['group_hit_rate_at_100']:.{precision}f} & {ind_dict['group_hit_rate_at_10']:.{precision}f} & {ind_dict['group_hit_rate_at_100']:.{precision}f} \\\\\\\\\\\\hline\"\n",
    "    full_string = first_row + \"\\n\" + second_row + \"\\n\"\n",
    "    print(full_string)\n",
    "    pyperclip.copy(full_string)\n",
    "\n",
    "all_dict = {column: user_splits_df[column].mean() for column in HIT_RATE_COLUMNS}\n",
    "ind_dict = {column: user_splits_unique_df[column].mean() for column in HIT_RATE_COLUMNS}\n",
    "\n",
    "format_dicts_into_latex(all_dict, ind_dict, precision=4, run_name=\"Tag Embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_outfit_category(tag_categories, tags, category):\n",
    "    tag_categories, tags = np.array(tag_categories), np.array(tags)\n",
    "    category_indexes = np.where(tag_categories == category)[0]\n",
    "    if len(category_indexes) == 0:\n",
    "        return \"\"\n",
    "    cat_tags = tags[category_indexes]\n",
    "    output = str(cat_tags[0])\n",
    "    return output\n",
    "\n",
    "outfits_df[\"size\"] = outfits_df.apply(lambda x: get_outfit_category(x[\"tag_categories\"], x[\"outfit_tags\"], \"Size\"), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfits_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
